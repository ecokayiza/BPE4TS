# Time Series BPE Tokenizer Experiment (æ—¶é—´åºåˆ— BPE åˆ†è¯å™¨å®éªŒ)

This project implements and evaluates a **Byte Pair Encoding (BPE)** tokenizer adapted for Time Series data. By discretizing continuous time series values into symbols and then applying BPE, we can compress the series into meaningful "motifs" (tokens) of variable lengths.

Methodology based on: *Time Series Tokenization via BPE (Conceptual)*.

## ğŸ“‚ Project Structure (é¡¹ç›®ç»“æ„)

```text
d:\Projects\BPE
â”œâ”€â”€ data/                   # Dataset folder (ETTm1.csv)
â”œâ”€â”€ result/                 # Experiment results & visualizations
â”‚   â”œâ”€â”€ detailed_tokenization.png  # Viz of reconstruction & token segments
â”‚   â”œâ”€â”€ pareto_frontier.png        # Compression vs MSE trade-off plot
â”‚   â”œâ”€â”€ tokenization_gallery.png   # Gallery of tokenization examples
â”‚   â””â”€â”€ experiment_grid_results.csv # All metrics
â”œâ”€â”€ src/                    # Source code
â”‚   â”œâ”€â”€ main.py             # Entry point (2-stage search pipeline)
â”‚   â”œâ”€â”€ data_loader.py      # Data downloading & loading
â”‚   â”œâ”€â”€ discretizer.py      # SAX-like discretization (Uniform/Quantile/Gaussian)
â”‚   â”œâ”€â”€ tokenizer.py        # BPE implementation (Train/Encode/Decode)
â”‚   â””â”€â”€ visualization.py    # Plotting utilities
â”œâ”€â”€ uv.lock                 # Dependency lock file
â””â”€â”€ pyproject.toml          # Project configuration
```

## ğŸš€ Quick Start (å¿«é€Ÿå¼€å§‹)

Make sure you have [uv](https://github.com/astral-sh/uv) installed.

1. **Run the Experiment (è¿è¡Œå®éªŒ)**:
   This will download data, preprocess, run a 2-stage (Coarse-to-Fine) parameter search, and generate visualizations.
   ```bash
   uv run -m src.main
   ```

2. **Check Results (æŸ¥çœ‹ç»“æœ)**:
   Go to the `result/` folder to see the generated plots and CSV.

## ğŸ”¬ Methodology (æ–¹æ³•è®º)

1.  **Preprocessing (é¢„å¤„ç†)**:
    - Z-Score Normalization (Zero Mean, Unit Var).
    - Truncation to range $[-5, 5]$.

2.  **Discretization (ç¦»æ•£åŒ–)**:
    - Mapping continuous values to discrete symbols using:
        - `quantile`: Empirical distribution (SAX-like).
        - `gaussian`: Theoretical Normal distribution.
        - `uniform_fixed`: Fixed intervals in $[-5, 5]$.

3.  **BPE Tokenization (BPE åˆ†è¯)**:
    - Iteratively merges the most frequent adjacent symbol pairs into new tokens.
    - Result: A vocabulary of variable-length motifs representing shapes like "rise", "fall", "peak".

4.  **Optimization (ä¼˜åŒ–ç­–ç•¥)**:
    - **Metric**: "Distance to Ideal" heuristic ($\sqrt{MSE_{norm}^2 + (1-Compression_{norm})^2}$).
    - **Strategy**: 2-Stage Search (Coarse Grid -> Fine Local Search).

## ğŸ“Š Experiment Results (å®éªŒç»“æœ)

**Dataset**: ETTm1 (Univariate 'OT' column - Oil Temperature).

### Best Configuration (æœ€ä½³æ¨¡å‹é…ç½®)
After an automated Coarse-to-Fine search:
- **Bins**: **55**
- **Strategy**: **Uniform Fixed**
- **Min Frequency**: **2**

### Performance Metrics (æ€§èƒ½æŒ‡æ ‡)
- **Reconstruction MSE**: **0.002784** (Very high fidelity)
- **Compression Ratio**: **~8.9x** (Significant reduction in sequence length)

### Visualizations (å¯è§†åŒ–è§£è¯»)

1.  **`pareto_frontier.png`**:
    - Shows the trade-off between **MSE** (x-axis) and **Compression Ratio** (y-axis).
    - **Insight**: `uniform_fixed` strategy generally yields higher compression but slightly higher error than `quantile`.

2.  **`detailed_tokenization.png`**:
    - **Top**: Red dashed line (Reconstructed) closely follows Black line (Original).
    - **Middle**: Shows actual **Tokens**. Long colored segments indicate the model learned long-term patterns.
    - **Bottom**: Histogram of token durations.

3.  **`tokenization_gallery.png`**:
    - A gallery of 8 distinct time windows, showing how the tokenizer adapts to different data shapes (trends, noise, seasonality) across the dataset.

## ğŸ“ Key Findings (ä¸»è¦å‘ç°)

1.  **Uniform Strategy Wins**: on this normalized dataset, simple uniform binning proved more robust for balancing compression and error compared to quantile binning (which is often too granular).
2.  **Adaptive Length**: The BPE successfully identified motifs of varying lengths. Stable regions are compressed into single "long tokens", while noisy regions use "short tokens".
3.  **Fine-Tuning Matters**: The 2-stage search successfully refined the bin count from a coarse 50 to a precise 55, improving the trade-off metrics.
